#!/usr/bin/env python3
"""
3ã¤ã®APIã‚«ã‚¿ãƒ­ã‚°ã‹ã‚‰å…¨PDFãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
"""

import asyncio
import base64
import json
import os
import re
from pathlib import Path
from typing import Dict, List, Set
from urllib.parse import urljoin, urlparse

import aiohttp
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()

BASE_URL = "https://city-api-catalog.smartcity-pf.com/yaizu"
EMAIL = os.getenv("YAIZU_API_EMAIL")
PASSWORD = os.getenv("YAIZU_API_PASSWORD")
DATA_DIR = Path("data/documentation")

# 3ã¤ã®API
API_CATALOGS = [
    {
        "name": "è¦³å…‰ãƒ»ç”£æ¥­API",
        "type": "tourism_industry",
        "description": "FIWARE NGSI v2"
    },
    {
        "name": "å…¬å…±æ–½è¨­API", 
        "type": "public_facility",
        "description": "FIWARE NGSI v2"
    },
    {
        "name": "é˜²ç½æƒ…å ±API",
        "type": "disaster_info", 
        "description": "FIWARE NGSI v2"
    }
]


class APIDocumentDownloader:
    def __init__(self):
        self.session = None
        self.auth_headers = {}
        self.pdf_urls = set()
        self.downloaded_files = []
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def authenticate(self):
        """Basicèªè¨¼ã§ãƒ­ã‚°ã‚¤ãƒ³"""
        credentials = f"{EMAIL}:{PASSWORD}"
        encoded = base64.b64encode(credentials.encode()).decode()
        self.auth_headers = {
            "Authorization": f"Basic {encoded}",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "User-Agent": "Mozilla/5.0"
        }
        
        # èªè¨¼ç¢ºèª
        async with self.session.get(f"{BASE_URL}/documentation", headers=self.auth_headers) as resp:
            return resp.status == 200
    
    async def extract_api_detail_urls(self) -> List[Dict]:
        """documentationãƒšãƒ¼ã‚¸ã‹ã‚‰å„APIã®è©³ç´°URLã‚’å–å¾—"""
        print("\nğŸ” APIã‚«ã‚¿ãƒ­ã‚°ã®è©³ç´°URLã‚’å–å¾—ä¸­...")
        
        async with self.session.get(f"{BASE_URL}/documentation", headers=self.auth_headers) as resp:
            if resp.status != 200:
                print(f"âŒ documentationãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“: {resp.status}")
                return []
            
            html = await resp.text()
            soup = BeautifulSoup(html, 'lxml')
            
            api_details = []
            
            # data-jsonå±æ€§ã‹ã‚‰APIæƒ…å ±ã‚’æŠ½å‡º
            catalog_items = soup.find_all(attrs={'data-json': True})
            
            for item in catalog_items:
                try:
                    data = json.loads(item.get('data-json'))
                    info = data.get('info', {})
                    title = info.get('title', '')
                    
                    # APIã®ç¨®é¡ã‚’ç‰¹å®š
                    api_type = None
                    if 'è¦³å…‰' in title or 'ç”£æ¥­' in title:
                        api_type = 'tourism_industry'
                    elif 'å…¬å…±æ–½è¨­' in title:
                        api_type = 'public_facility'
                    elif 'é˜²ç½' in title:
                        api_type = 'disaster_info'
                    
                    if api_type:
                        # è©³ç´°ãƒšãƒ¼ã‚¸ã®URLã‚’æ§‹ç¯‰
                        # Kong Portalã§ã¯é€šå¸¸ /documentation/{service_id} ã®å½¢å¼
                        service_id = data.get('id') or data.get('name', '').lower().replace(' ', '-')
                        detail_url = f"{BASE_URL}/documentation/{service_id}"
                        
                        api_details.append({
                            'name': title,
                            'type': api_type,
                            'detail_url': detail_url,
                            'data': data
                        })
                        
                        print(f"  âœ… {title}")
                        print(f"     URL: {detail_url}")
                
                except Exception as e:
                    print(f"  âš ï¸ ãƒ‡ãƒ¼ã‚¿è§£æã‚¨ãƒ©ãƒ¼: {e}")
            
            # HTMLãƒªãƒ³ã‚¯ã‹ã‚‰ã‚‚æ¢ç´¢
            links = soup.find_all('a', href=True)
            for link in links:
                href = link.get('href')
                text = link.get_text(strip=True)
                
                # APIè©³ç´°ãƒšãƒ¼ã‚¸ã£ã½ã„ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                if href and any(api_name in text for api in API_CATALOGS for api_name in [api['name']]):
                    full_url = urljoin(f"{BASE_URL}/", href)
                    
                    # æ—¢ã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã§ãªã„å ´åˆ
                    if not any(detail['detail_url'] == full_url for detail in api_details):
                        api_details.append({
                            'name': text,
                            'type': 'unknown',
                            'detail_url': full_url,
                            'data': {}
                        })
                        print(f"  ğŸ“ ãƒªãƒ³ã‚¯ç™ºè¦‹: {text} -> {full_url}")
            
            return api_details
    
    async def explore_api_detail_page(self, api_info: Dict):
        """å„APIã®è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰PDFã‚„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—"""
        print(f"\nğŸ“– {api_info['name']} ã®è©³ç´°ã‚’æ¢ç´¢ä¸­...")
        print(f"   URL: {api_info['detail_url']}")
        
        try:
            async with self.session.get(api_info['detail_url'], headers=self.auth_headers) as resp:
                print(f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {resp.status}")
                
                if resp.status != 200:
                    print(f"   âš ï¸ ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“")
                    return []
                
                html = await resp.text()
                soup = BeautifulSoup(html, 'lxml')
                
                pdf_urls = []
                
                # 1. ç›´æ¥çš„ãªPDFãƒªãƒ³ã‚¯ã‚’æ¢ã™
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    text = link.get_text(strip=True)
                    
                    if href.lower().endswith('.pdf') or '.pdf' in href.lower():
                        full_url = urljoin(api_info['detail_url'], href)
                        pdf_urls.append({
                            'url': full_url,
                            'name': text or f"{api_info['type']}_document",
                            'type': 'direct_link'
                        })
                
                # 2. OpenAPI/Swaggerä»•æ§˜æ›¸ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯
                spec_patterns = [
                    r'download.*(?:openapi|swagger|spec)',
                    r'(?:openapi|swagger|spec).*download',
                    r'api.*spec.*pdf',
                    r'specification.*pdf'
                ]
                
                for pattern in spec_patterns:
                    for element in soup.find_all(text=re.compile(pattern, re.I)):
                        parent = element.parent
                        if parent and parent.name == 'a' and parent.get('href'):
                            full_url = urljoin(api_info['detail_url'], parent['href'])
                            pdf_urls.append({
                                'url': full_url,
                                'name': f"{api_info['type']}_openapi_spec",
                                'type': 'openapi_spec'
                            })
                
                # 3. Kong Portalç‰¹æœ‰ã®ä»•æ§˜æ›¸ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
                # /specs/{service_id} ã®ã‚ˆã†ãªã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’è©¦ã™
                service_id = api_info.get('data', {}).get('id') or api_info['type']
                spec_endpoints = [
                    f"{BASE_URL.replace('city-api-catalog', 'city-api-catalog-api')}/specs/{service_id}",
                    f"{BASE_URL}/specs/{service_id}",
                    f"{api_info['detail_url']}/spec",
                    f"{api_info['detail_url']}/download"
                ]
                
                for endpoint in spec_endpoints:
                    try:
                        async with self.session.get(endpoint, headers=self.auth_headers) as spec_resp:
                            if spec_resp.status == 200:
                                content_type = spec_resp.headers.get('content-type', '')
                                
                                if 'application/pdf' in content_type:
                                    pdf_urls.append({
                                        'url': endpoint,
                                        'name': f"{api_info['type']}_specification",
                                        'type': 'api_endpoint'
                                    })
                                elif 'application/json' in content_type or 'application/yaml' in content_type:
                                    # OpenAPI/Swaggerã®å ´åˆã€PDFã«å¤‰æ›å¯èƒ½ãªãƒªãƒ³ã‚¯ãŒã‚ã‚‹ã‹ç¢ºèª
                                    spec_data = await spec_resp.text()
                                    if 'openapi' in spec_data.lower() or 'swagger' in spec_data.lower():
                                        # ä»•æ§˜æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆå¾Œã§PDFå¤‰æ›ç”¨ï¼‰
                                        spec_file = DATA_DIR / f"{api_info['type']}_openapi.json"
                                        with open(spec_file, 'w', encoding='utf-8') as f:
                                            f.write(spec_data)
                                        print(f"   ğŸ’¾ OpenAPIä»•æ§˜ä¿å­˜: {spec_file}")
                    except:
                        continue
                
                # 4. JavaScriptã‹ã‚‰åŸ‹ã‚è¾¼ã¾ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆURLã‚’æ¢ã™
                for script in soup.find_all('script'):
                    if script.string:
                        # PDF URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
                        pdf_patterns = re.findall(
                            r'["\']([^"\']*\.pdf[^"\']*)["\']', 
                            script.string, 
                            re.IGNORECASE
                        )
                        for pattern in pdf_patterns:
                            if not pattern.startswith('http'):
                                pattern = urljoin(api_info['detail_url'], pattern)
                            pdf_urls.append({
                                'url': pattern,
                                'name': f"{api_info['type']}_embedded",
                                'type': 'javascript_embedded'
                            })
                
                print(f"   ğŸ“„ PDFãƒªãƒ³ã‚¯ç™ºè¦‹: {len(pdf_urls)} å€‹")
                for pdf in pdf_urls:
                    print(f"     - {pdf['name']}: {pdf['url']}")
                
                return pdf_urls
                
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def download_pdf(self, pdf_info: Dict, api_name: str):
        """PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        url = pdf_info['url']
        base_name = pdf_info['name']
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        safe_api_name = re.sub(r'[^a-zA-Z0-9_-]', '_', api_name)[:20]
        safe_doc_name = re.sub(r'[^a-zA-Z0-9_-]', '_', base_name)[:30]
        filename = f"{safe_api_name}_{safe_doc_name}.pdf"
        filepath = DATA_DIR / filename
        
        # æ—¢å­˜ãƒã‚§ãƒƒã‚¯
        if filepath.exists():
            print(f"    â­ï¸ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å­˜ï¼‰: {filename}")
            return True
        
        print(f"    ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {filename}")
        print(f"       URL: {url}")
        
        try:
            async with self.session.get(url, headers=self.auth_headers) as resp:
                if resp.status == 200:
                    content = await resp.read()
                    
                    # PDFã‹ã©ã†ã‹ç¢ºèª
                    if content[:4] == b'%PDF':
                        with open(filepath, 'wb') as f:
                            f.write(content)
                        
                        size_kb = len(content) / 1024
                        print(f"    âœ… ä¿å­˜å®Œäº†: {filepath} ({size_kb:.1f} KB)")
                        self.downloaded_files.append({
                            'filename': filename,
                            'api': api_name,
                            'size_kb': size_kb,
                            'url': url
                        })
                        return True
                    else:
                        print(f"    âš ï¸ PDFã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆContent-Type: {resp.headers.get('content-type', 'unknown')}ï¼‰")
                        
                        # HTMLã‚„JSONã®å ´åˆã€PDFã¸ã®ç›´æ¥ãƒªãƒ³ã‚¯ãŒãªã„ã‹ç¢ºèª
                        if resp.headers.get('content-type', '').startswith('text/html'):
                            html_content = content.decode('utf-8', errors='ignore')
                            soup_content = BeautifulSoup(html_content, 'lxml')
                            
                            # PDFç›´æ¥ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                            pdf_links = soup_content.find_all('a', href=lambda x: x and '.pdf' in x.lower())
                            if pdf_links:
                                print(f"    ğŸ“ HTMLã‹ã‚‰{len(pdf_links)}å€‹ã®PDFãƒªãƒ³ã‚¯ç™ºè¦‹")
                                for link in pdf_links[:3]:
                                    href = link.get('href')
                                    if href:
                                        nested_url = urljoin(url, href)
                                        await self.download_pdf({
                                            'url': nested_url,
                                            'name': f"nested_{base_name}"
                                        }, api_name)
                        
                        return False
                else:
                    print(f"    âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {resp.status}")
                    return False
                    
        except Exception as e:
            print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def process_all_apis(self):
        """å…¨APIã‚’å‡¦ç†"""
        print("="*60)
        print("ç„¼æ´¥å¸‚APIã‚«ã‚¿ãƒ­ã‚° - å…¨PDFå–å¾—")
        print("="*60)
        
        if not await self.authenticate():
            print("âŒ èªè¨¼å¤±æ•—")
            return
        
        print("âœ… èªè¨¼æˆåŠŸ")
        
        # APIè©³ç´°URLã‚’å–å¾—
        api_details = await self.extract_api_detail_urls()
        
        if not api_details:
            print("âŒ APIè©³ç´°æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¢çŸ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§è©¦è¡Œ
            api_details = [
                {
                    'name': 'è¦³å…‰ãƒ»ç”£æ¥­APIï¼ˆFIWARE NGSI v2ï¼‰',
                    'type': 'tourism_industry', 
                    'detail_url': f"{BASE_URL}/documentation/tourism-industry-api"
                },
                {
                    'name': 'å…¬å…±æ–½è¨­APIï¼ˆFIWARE NGSI v2ï¼‰',
                    'type': 'public_facility',
                    'detail_url': f"{BASE_URL}/documentation/public-facility-api"  
                },
                {
                    'name': 'é˜²ç½æƒ…å ±APIï¼ˆFIWARE NGSI v2ï¼‰',
                    'type': 'disaster_info',
                    'detail_url': f"{BASE_URL}/documentation/disaster-info-api"
                }
            ]
        
        # å„APIã‚’å‡¦ç†
        for api in api_details:
            print(f"\n" + "="*50)
            print(f"ğŸ“‹ {api['name']} ã®å‡¦ç†")
            print("="*50)
            
            # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰PDFã‚’æ¢ç´¢
            pdf_list = await self.explore_api_detail_page(api)
            
            # PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            if pdf_list:
                print(f"\nğŸ“¥ PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {len(pdf_list)} ãƒ•ã‚¡ã‚¤ãƒ«")
                for pdf in pdf_list:
                    await self.download_pdf(pdf, api['name'])
                    await asyncio.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            else:
                print("\nâš ï¸ PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            # è¿½åŠ : Kong APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ç›´æ¥å–å¾—ã‚’è©¦è¡Œ
            await self.try_direct_api_access(api)
    
    async def extract_api_detail_urls(self) -> List[Dict]:
        """documentationãƒšãƒ¼ã‚¸ã‹ã‚‰APIè©³ç´°URLã‚’æŠ½å‡º"""
        async with self.session.get(f"{BASE_URL}/documentation", headers=self.auth_headers) as resp:
            html = await resp.text()
            soup = BeautifulSoup(html, 'lxml')
            
            api_details = []
            
            # data-jsonå±æ€§ã‚’æŒã¤è¦ç´ ã‹ã‚‰æŠ½å‡º
            catalog_items = soup.find_all(attrs={'data-json': True})
            
            for item in catalog_items:
                try:
                    data = json.loads(item.get('data-json'))
                    title = data.get('info', {}).get('title', '')
                    
                    if title:
                        # ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªè¦ç´ ã‚’æ¢ã™
                        clickable = item.find('a', href=True)
                        if clickable:
                            detail_url = urljoin(f"{BASE_URL}/", clickable['href'])
                        else:
                            # IDã‹ã‚‰æ¨æ¸¬
                            service_id = data.get('id', title.lower().replace(' ', '-').replace('ï¼ˆ', '').replace('ï¼‰', ''))
                            detail_url = f"{BASE_URL}/documentation/{service_id}"
                        
                        api_details.append({
                            'name': title,
                            'type': self._classify_api_type(title),
                            'detail_url': detail_url,
                            'data': data
                        })
                
                except Exception as e:
                    continue
            
            return api_details
    
    def _classify_api_type(self, title: str) -> str:
        """APIã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ç¨®é¡ã‚’åˆ†é¡"""
        title_lower = title.lower()
        if 'è¦³å…‰' in title_lower or 'ç”£æ¥­' in title_lower:
            return 'tourism_industry'
        elif 'å…¬å…±æ–½è¨­' in title_lower:
            return 'public_facility'
        elif 'é˜²ç½' in title_lower:
            return 'disaster_info'
        else:
            return 'unknown'
    
    async def explore_api_detail_page(self, api_info: Dict) -> List[Dict]:
        """APIè©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰PDF URLã‚’å–å¾—"""
        try:
            async with self.session.get(api_info['detail_url'], headers=self.auth_headers) as resp:
                if resp.status != 200:
                    return []
                
                html = await resp.text()
                soup = BeautifulSoup(html, 'lxml')
                
                pdf_urls = []
                
                # PDFãƒªãƒ³ã‚¯ã‚’æ¢ã™
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    text = link.get_text(strip=True)
                    
                    if '.pdf' in href.lower() or any(keyword in text.lower() 
                                                   for keyword in ['pdf', 'download', 'spec', 'ä»•æ§˜æ›¸', 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰']):
                        full_url = urljoin(api_info['detail_url'], href)
                        pdf_urls.append({
                            'url': full_url,
                            'name': text or 'document',
                            'type': 'detail_page'
                        })
                
                # Swagger UI ã‹ã‚‰ä»•æ§˜æ›¸ã‚’æ¢ã™
                swagger_elements = soup.find_all(['div', 'section'], class_=re.compile(r'swagger|openapi'))
                for element in swagger_elements:
                    # dataå±æ€§ã‹ã‚‰ä»•æ§˜æ›¸URLã‚’å–å¾—
                    spec_url = element.get('data-url') or element.get('data-spec-url')
                    if spec_url:
                        full_url = urljoin(api_info['detail_url'], spec_url)
                        pdf_urls.append({
                            'url': full_url,
                            'name': f"{api_info['type']}_swagger_spec",
                            'type': 'swagger_spec'
                        })
                
                return pdf_urls
                
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    async def try_direct_api_access(self, api_info: Dict):
        """Kong APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã‚’è©¦è¡Œ"""
        print(f"\nğŸ”§ {api_info['name']} - ç›´æ¥APIã‚¢ã‚¯ã‚»ã‚¹è©¦è¡Œ")
        
        api_base = BASE_URL.replace('city-api-catalog', 'city-api-catalog-api')
        service_id = api_info.get('data', {}).get('id') or api_info['type']
        
        direct_endpoints = [
            f"{api_base}/services/{service_id}/documentation",
            f"{api_base}/files?tags={service_id}",
            f"{api_base}/specs/{service_id}",
        ]
        
        for endpoint in direct_endpoints:
            try:
                async with self.session.get(endpoint, headers=self.auth_headers) as resp:
                    if resp.status == 200:
                        content_type = resp.headers.get('content-type', '')
                        
                        if 'application/pdf' in content_type:
                            # ç›´æ¥PDFãŒè¿”ã•ã‚Œã‚‹å ´åˆ
                            await self.download_pdf({
                                'url': endpoint,
                                'name': f'direct_{service_id}'
                            }, api_info['name'])
                        
                        elif 'application/json' in content_type:
                            data = await resp.json()
                            print(f"     âœ… JSONãƒ‡ãƒ¼ã‚¿å–å¾—: {endpoint}")
                            
                            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‹ã‚‰ PDFã‚’æ¢ã™
                            if isinstance(data, dict) and 'data' in data:
                                files = data['data']
                                if isinstance(files, list):
                                    pdf_files = [f for f in files 
                                               if isinstance(f, dict) and 
                                               (f.get('path', '').lower().endswith('.pdf') or 
                                                'pdf' in f.get('contents', '').lower())]
                                    
                                    if pdf_files:
                                        print(f"       ğŸ“„ PDFãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {len(pdf_files)} å€‹")
                                        for pdf_file in pdf_files:
                                            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                                            if 'contents' in pdf_file and pdf_file['contents'].startswith('%PDF'):
                                                filename = f"{service_id}_{pdf_file.get('path', 'file').replace('/', '_')}"
                                                filepath = DATA_DIR / filename
                                                
                                                with open(filepath, 'w', encoding='utf-8') as f:
                                                    f.write(pdf_file['contents'])
                                                print(f"       ğŸ’¾ ä¿å­˜: {filepath}")
            except:
                continue


async def main():
    async with APIDocumentDownloader() as downloader:
        await downloader.process_all_apis()
        
        # çµæœãƒ¬ãƒãƒ¼ãƒˆ
        print("\n" + "="*60)
        print("ğŸ“Š ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        
        if downloader.downloaded_files:
            print(f"\nâœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«: {len(downloader.downloaded_files)} å€‹")
            
            total_size = 0
            for file_info in downloader.downloaded_files:
                print(f"  ğŸ“„ {file_info['filename']}")
                print(f"     API: {file_info['api']}")
                print(f"     ã‚µã‚¤ã‚º: {file_info['size_kb']:.1f} KB")
                print(f"     URL: {file_info['url']}")
                total_size += file_info['size_kb']
            
            print(f"\nğŸ“Š åˆè¨ˆã‚µã‚¤ã‚º: {total_size:.1f} KB")
        else:
            print("\nâš ï¸ æ–°ã—ã„PDFãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
        all_files = list(DATA_DIR.glob("*.pdf"))
        print(f"\nğŸ“ data/documentation ã®å…¨PDFãƒ•ã‚¡ã‚¤ãƒ«: {len(all_files)} å€‹")
        for pdf_file in sorted(all_files):
            size_mb = pdf_file.stat().st_size / (1024 * 1024)
            print(f"  - {pdf_file.name} ({size_mb:.2f} MB)")


if __name__ == "__main__":
    asyncio.run(main())