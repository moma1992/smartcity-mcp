#!/usr/bin/env python3
"""
PDFè§£æã‹ã‚‰JSONç”Ÿæˆãƒ„ãƒ¼ãƒ«

PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’è©³ç´°è§£æã—ã¦ã€ãƒªãƒƒãƒãªJSONãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆ
"""

import asyncio
import json
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# PDFè§£æãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import PyPDF2
    import fitz  # PyMuPDF
    PDF_LIBRARIES_AVAILABLE = True
except ImportError:
    print("âš ï¸ PDFè§£æãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŸºæœ¬çš„ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ç”Ÿæˆã—ã¾ã™ã€‚")
    PDF_LIBRARIES_AVAILABLE = False


class PDFToJSONGenerator:
    """PDFè§£æã‹ã‚‰JSONç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.documentation_dir = Path("data/documentation")
        self.api_specs_dir = Path("data/api_specs")
        self.api_specs_dir.mkdir(parents=True, exist_ok=True)
        
        # OpenAPIã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ”ãƒ³ã‚°
        self.category_mapping = {
            "bousai-api": "é˜²ç½æƒ…å ±API",
            "public-facility-api": "å…¬å…±æ–½è¨­API", 
            "tourism-api": "è¦³å…‰ãƒ»ç”£æ¥­API"
        }
    
    def extract_text_from_pdf(self, pdf_path: Path) -> str:
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        if not PDF_LIBRARIES_AVAILABLE:
            return ""
            
        text = ""
        try:
            # PyMuPDFã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            doc = fitz.open(pdf_path)
            for page in doc:
                text += page.get_text()
            doc.close()
        except Exception as e:
            print(f"  âš ï¸ PDFèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {pdf_path.name} - {e}")
            try:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šPyPDF2ã‚’è©¦ã™
                with open(pdf_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    for page in reader.pages:
                        text += page.extract_text()
            except Exception as e2:
                print(f"  âŒ PDFèª­ã¿è¾¼ã¿å¤±æ•—: {pdf_path.name} - {e2}")
        
        return text
    
    def analyze_pdf_content(self, pdf_path: Path, text: str) -> Dict[str, Any]:
        """PDFã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è§£æã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        entity_name = pdf_path.stem
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        analysis = {
            "entity_type": entity_name,
            "extracted_fields": self._extract_field_information(text),
            "data_types": self._identify_data_types(text),
            "relationships": self._find_relationships(text),
            "constraints": self._extract_constraints(text),
            "examples": self._extract_examples(text),
            "description": self._generate_description(entity_name, text)
        }
        
        return analysis
    
    def _extract_field_information(self, text: str) -> List[Dict[str, Any]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æƒ…å ±ã‚’æŠ½å‡º"""
        fields = []
        
        # æ—¥æœ¬èªã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        field_patterns = [
            r'([a-zA-Z_][a-zA-Z0-9_]*)\s*:\s*([^\n]+)',  # field: description
            r'ãƒ»\s*([^ï¼š\n]+)ï¼š([^\n]+)',  # ãƒ»ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åï¼šèª¬æ˜
            r'é …ç›®å\s*[:ï¼š]\s*([^\n]+)',  # é …ç›®å: å€¤
            r'å±æ€§\s*[:ï¼š]\s*([^\n]+)',   # å±æ€§: å€¤
        ]
        
        for pattern in field_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                field_name, description = match
                field_name = field_name.strip()
                description = description.strip()
                
                if len(field_name) > 1 and len(description) > 1:
                    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ¨å®š
                    data_type = self._guess_data_type(field_name, description)
                    
                    fields.append({
                        "name": self._normalize_field_name(field_name),
                        "description": description[:200],  # èª¬æ˜ã‚’200æ–‡å­—ã«åˆ¶é™
                        "type": data_type,
                        "required": self._is_required_field(field_name, description)
                    })
        
        return fields[:10]  # æœ€å¤§10ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«åˆ¶é™
    
    def _normalize_field_name(self, name: str) -> str:
        """ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’æ­£è¦åŒ–"""
        # æ—¥æœ¬èªã‚’ãƒ­ãƒ¼ãƒå­—ã«å¤‰æ›ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        replacements = {
            'åå‰': 'name',
            'åç§°': 'name', 
            'ä½æ‰€': 'address',
            'ä½ç½®': 'location',
            'åº§æ¨™': 'coordinates',
            'ID': 'id',
            'è­˜åˆ¥å­': 'id',
            'ç¨®åˆ¥': 'type',
            'åˆ†é¡': 'category',
            'çŠ¶æ…‹': 'status',
            'çŠ¶æ³': 'status',
            'æ—¥æ™‚': 'dateTime',
            'æ™‚åˆ»': 'time',
            'å€¤': 'value',
            'å®¹é‡': 'capacity',
            'äººæ•°': 'capacity'
        }
        
        for jp, en in replacements.items():
            if jp in name:
                return en
                
        # è‹±æ•°å­—ã®ã¿ã«å¤‰æ›
        normalized = re.sub(r'[^a-zA-Z0-9]', '_', name)
        normalized = re.sub(r'_+', '_', normalized).strip('_')
        
        return normalized.lower() if normalized else 'unknown_field'
    
    def _guess_data_type(self, field_name: str, description: str) -> str:
        """ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã¨èª¬æ˜ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ¨å®š"""
        text = (field_name + " " + description).lower()
        
        if any(keyword in text for keyword in ['åº§æ¨™', 'coordinate', 'ç·¯åº¦', 'çµŒåº¦', 'location']):
            return 'geo:json'
        elif any(keyword in text for keyword in ['æ—¥æ™‚', 'datetime', 'æ™‚åˆ»', 'time', 'æ—¥ä»˜', 'date']):
            return 'DateTime'
        elif any(keyword in text for keyword in ['æ•°', 'number', 'å€¤', 'value', 'å®¹é‡', 'capacity', 'äººæ•°']):
            return 'Number'
        elif any(keyword in text for keyword in ['true', 'false', 'boolean', 'æœ‰ç„¡', 'ãƒ•ãƒ©ã‚°']):
            return 'Boolean'
        elif 'id' in text or 'è­˜åˆ¥' in text:
            return 'Text'
        else:
            return 'Text'
    
    def _is_required_field(self, field_name: str, description: str) -> bool:
        """å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        text = (field_name + " " + description).lower()
        
        # å¿…é ˆã¨æ€ã‚ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        required_keywords = ['å¿…é ˆ', 'required', 'id', 'è­˜åˆ¥å­', 'type', 'ç¨®åˆ¥']
        optional_keywords = ['ä»»æ„', 'optional', 'å¯èƒ½', 'ã‚ªãƒ—ã‚·ãƒ§ãƒ³']
        
        if any(keyword in text for keyword in required_keywords):
            return True
        elif any(keyword in text for keyword in optional_keywords):
            return False
        else:
            # IDã‚„typeã£ã½ã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯å¿…é ˆã¨ã™ã‚‹
            return field_name.lower() in ['id', 'type', 'name', 'è­˜åˆ¥å­', 'ç¨®åˆ¥', 'åç§°']
    
    def _identify_data_types(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç‰¹å®š"""
        types = set()
        
        if re.search(r'åº§æ¨™|ç·¯åº¦|çµŒåº¦|location|coordinate', text, re.IGNORECASE):
            types.add('geo:json')
        if re.search(r'æ—¥æ™‚|datetime|timestamp|æ™‚åˆ»', text, re.IGNORECASE):
            types.add('DateTime')  
        if re.search(r'æ•°å€¤|number|å€¤|å®¹é‡|äººæ•°', text, re.IGNORECASE):
            types.add('Number')
        if re.search(r'ä½æ‰€|address', text, re.IGNORECASE):
            types.add('PostalAddress')
        if re.search(r'URL|ãƒªãƒ³ã‚¯|link', text, re.IGNORECASE):
            types.add('URL')
            
        return list(types)
    
    def _find_relationships(self, text: str) -> List[str]:
        """ä»–ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨ã®é–¢ä¿‚ã‚’ç‰¹å®š"""
        relationships = []
        
        # é–¢é€£ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã‚’æ¤œç´¢
        entity_patterns = [
            r'é¿é›£æ‰€',
            r'é¿é›£å ´æ‰€', 
            r'é˜²ç½æ–½è¨­',
            r'åŒ»ç™‚æ©Ÿé–¢',
            r'AED',
            r'ã‚»ãƒ³ã‚µãƒ¼',
            r'ã‚«ãƒ¡ãƒ©',
            r'è¦³å…‰åœ°',
            r'ã‚¤ãƒ™ãƒ³ãƒˆ'
        ]
        
        for pattern in entity_patterns:
            if re.search(pattern, text):
                relationships.append(pattern)
                
        return relationships[:5]  # æœ€å¤§5ã¤ã«åˆ¶é™
    
    def _extract_constraints(self, text: str) -> Dict[str, Any]:
        """åˆ¶ç´„æ¡ä»¶ã‚’æŠ½å‡º"""
        constraints = {}
        
        # æ•°å€¤åˆ¶ç´„ã‚’æ¤œç´¢
        number_patterns = [
            r'æœ€å¤§(\d+)',
            r'æœ€å°(\d+)',
            r'ä¸Šé™(\d+)',
            r'ä¸‹é™(\d+)'
        ]
        
        for pattern in number_patterns:
            matches = re.findall(pattern, text)
            if matches:
                constraint_type = pattern.replace(r'(\d+)', '').replace('\\', '')
                constraints[constraint_type] = int(matches[0])
        
        return constraints
    
    def _extract_examples(self, text: str) -> List[str]:
        """ä¾‹ã‚„ã‚µãƒ³ãƒ—ãƒ«å€¤ã‚’æŠ½å‡º"""
        examples = []
        
        # ä¾‹ã‚’ç¤ºã™ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        example_patterns = [
            r'ä¾‹\s*[:ï¼š]\s*([^\n]+)',
            r'ã‚µãƒ³ãƒ—ãƒ«\s*[:ï¼š]\s*([^\n]+)',
            r'å…·ä½“ä¾‹\s*[:ï¼š]\s*([^\n]+)'
        ]
        
        for pattern in example_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                examples.append(match.strip()[:100])  # 100æ–‡å­—ã«åˆ¶é™
                
        return examples[:3]  # æœ€å¤§3ã¤ã®ä¾‹
    
    def _generate_description(self, entity_name: str, text: str) -> str:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®èª¬æ˜ã‚’ç”Ÿæˆ"""
        # ãƒ†ã‚­ã‚¹ãƒˆã®æœ€åˆã®æ•°è¡Œã‹ã‚‰èª¬æ˜ã‚’æŠ½å‡º
        lines = text.split('\n')[:10]  # æœ€åˆã®10è¡Œ
        description_text = ' '.join(line.strip() for line in lines if line.strip())
        
        if len(description_text) > 300:
            description_text = description_text[:300] + "..."
            
        return description_text if description_text else f"{entity_name}ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ç®¡ç†ã™ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£"
    
    def generate_enhanced_json_schema(self, pdf_path: Path, category: str) -> Dict[str, Any]:
        """PDFã‹ã‚‰æ‹¡å¼µJSONã‚¹ã‚­ãƒ¼ãƒã‚’ç”Ÿæˆ"""
        entity_type = pdf_path.stem
        entity_name_jp = self._get_japanese_name(entity_type)
        
        # PDFãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºãƒ»è§£æ
        pdf_text = self.extract_text_from_pdf(pdf_path)
        analysis = self.analyze_pdf_content(pdf_path, pdf_text)
        
        # åŸºæœ¬ã‚¹ã‚­ãƒ¼ãƒ
        schema = {
            "schema_version": "2.0.0",
            "generated_at": datetime.now().isoformat(),
            "source": {
                "pdf_file": pdf_path.name,
                "pdf_path": str(pdf_path),
                "api_category": self.category_mapping.get(category, category),
                "text_extraction_success": bool(pdf_text)
            },
            "entity": {
                "type": entity_type,
                "name_ja": entity_name_jp,
                "description": analysis["description"],
                "category": self._classify_entity_category(entity_type),
                "fiware_service": "smartcity_yaizu",
                "fiware_service_path": f"/{entity_type}"
            },
            "attributes": self._build_enhanced_attributes(analysis),
            "api_specification": {
                "base_url": "https://api.smartcity-yaizu.jp",
                "endpoints": self._generate_api_endpoints(entity_type),
                "required_headers": {
                    "Fiware-Service": "smartcity_yaizu",
                    "Fiware-ServicePath": f"/{entity_type}",
                    "Content-Type": "application/json"
                }
            },
            "usage_examples": self._generate_usage_examples(entity_type),
            "relationships": {
                "related_entities": analysis["relationships"],
                "potential_links": self._find_potential_entity_links(entity_type)
            },
            "metadata": {
                "data_quality": self._assess_data_quality(analysis),
                "completeness": len(analysis["extracted_fields"]) > 0,
                "last_updated": datetime.now().isoformat()
            }
        }
        
        return schema
    
    def _get_japanese_name(self, entity_type: str) -> str:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¿ã‚¤ãƒ—ã‹ã‚‰æ—¥æœ¬èªåã‚’æ¨å®š"""
        name_mappings = {
            "Aed": "AEDè¨­ç½®å ´æ‰€",
            "Event": "ã‚¤ãƒ™ãƒ³ãƒˆä¸€è¦§", 
            "EventDetail": "ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°",
            "PublicFacility": "å…¬å…±æ–½è¨­",
            "SightseeingMapStore": "è¦³å…‰æ–½è¨­ç­‰ä¸€è¦§",
            "FactoryDirectSalesPlace": "å·¥å ´ä½µè¨­ç›´å£²æ‰€",
            "WeatherAlert": "è­¦å ±ãƒ»æ³¨æ„å ±",
            "WeatherForecast": "å¤©å€™",
            "EvacuationShelter": "é¿é›£æ‰€é–‹è¨­çŠ¶æ³",
            "EvacuationSpace": "æŒ‡å®šç·Šæ€¥é¿é›£å ´æ‰€",
            "PrecipitationGauge": "é›¨é‡è¨ˆ",
            "StreamGauge": "æ²³å·æ°´ä½è¨ˆ",
            "CameraInformation": "æ²³å·ãƒ»æµ·å²¸ã‚«ãƒ¡ãƒ©"
        }
        
        return name_mappings.get(entity_type, entity_type)
    
    def _classify_entity_category(self, entity_type: str) -> List[str]:
        """ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã‚«ãƒ†ã‚´ãƒªã‚’åˆ†é¡"""
        categories = []
        
        disaster_keywords = ['Evacuation', 'Disaster', 'Weather', 'Alert', 'Flood', 'Tsunami']
        infrastructure_keywords = ['Facility', 'Tank', 'Warehouse', 'Station', 'Building']
        environmental_keywords = ['Gauge', 'Sensor', 'Camera', 'Information']
        tourism_keywords = ['Event', 'Sightseeing', 'Tourism', 'Factory']
        medical_keywords = ['Aed', 'Hospital', 'Aid', 'Relief']
        
        entity_lower = entity_type.lower()
        
        if any(keyword.lower() in entity_lower for keyword in disaster_keywords):
            categories.append("disaster_management")
        if any(keyword.lower() in entity_lower for keyword in infrastructure_keywords):
            categories.append("infrastructure") 
        if any(keyword.lower() in entity_lower for keyword in environmental_keywords):
            categories.append("environmental")
        if any(keyword.lower() in entity_lower for keyword in tourism_keywords):
            categories.append("tourism_industry")
        if any(keyword.lower() in entity_lower for keyword in medical_keywords):
            categories.append("medical_emergency")
            
        return categories if categories else ["general"]
    
    def _build_enhanced_attributes(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ‹¡å¼µå±æ€§ã‚’æ§‹ç¯‰"""
        attributes = {
            # æ¨™æº–FIWAREå±æ€§
            "id": {
                "type": "Text",
                "description": "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ä¸€æ„è­˜åˆ¥å­",
                "required": True,
                "format": "uri",
                "example": f"urn:ngsi-ld:{analysis['entity_type']}:001"
            },
            "type": {
                "type": "Text",
                "description": "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚¿ã‚¤ãƒ—",
                "required": True,
                "constant": analysis['entity_type']
            },
            "dateCreated": {
                "type": "DateTime", 
                "description": "ä½œæˆæ—¥æ™‚",
                "required": False,
                "format": "date-time"
            },
            "dateModified": {
                "type": "DateTime",
                "description": "æ›´æ–°æ—¥æ™‚", 
                "required": False,
                "format": "date-time"
            }
        }
        
        # ä½ç½®æƒ…å ±ï¼ˆã»ã¨ã‚“ã©ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«å­˜åœ¨ï¼‰
        attributes["location"] = {
            "type": "geo:json",
            "description": "åœ°ç†çš„ä½ç½®æƒ…å ±",
            "required": False,
            "properties": {
                "type": {"type": "Text", "enum": ["Point"]},
                "coordinates": {"type": "Array", "items": "Number", "minItems": 2, "maxItems": 2}
            }
        }
        
        # ä½æ‰€æƒ…å ±
        attributes["address"] = {
            "type": "PostalAddress",
            "description": "ä½æ‰€æƒ…å ±", 
            "required": False,
            "properties": {
                "addressCountry": {"type": "Text", "default": "JP"},
                "addressRegion": {"type": "Text", "default": "é™å²¡çœŒ"},
                "addressLocality": {"type": "Text", "default": "ç„¼æ´¥å¸‚"},
                "streetAddress": {"type": "Text"}
            }
        }
        
        # PDFã‹ã‚‰æŠ½å‡ºã—ãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
        for field in analysis.get("extracted_fields", []):
            attributes[field["name"]] = {
                "type": field["type"],
                "description": field["description"],
                "required": field["required"]
            }
            
        return attributes
    
    def _generate_api_endpoints(self, entity_type: str) -> List[Dict[str, Any]]:
        """APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæƒ…å ±ã‚’ç”Ÿæˆ"""
        endpoints = [
            {
                "name": "å…¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å–å¾—",
                "method": "GET",
                "path": "/v2/entities",
                "description": f"å…¨ã¦ã®{entity_type}ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å–å¾—",
                "parameters": {
                    "type": entity_type,
                    "limit": {"type": "integer", "default": 100},
                    "offset": {"type": "integer", "default": 0}
                }
            },
            {
                "name": "IDæŒ‡å®šå–å¾—",
                "method": "GET", 
                "path": "/v2/entities/{entityId}",
                "description": f"ç‰¹å®šã®{entity_type}ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å–å¾—",
                "path_parameters": {
                    "entityId": {"type": "string", "description": "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ID"}
                }
            },
            {
                "name": "åœ°ç†çš„æ¤œç´¢",
                "method": "GET",
                "path": "/v2/entities",
                "description": f"åœ°ç†çš„ç¯„å›²å†…ã®{entity_type}ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ¤œç´¢",
                "parameters": {
                    "type": entity_type,
                    "georel": {"type": "string", "example": "near;maxDistance:1000"},
                    "geometry": {"type": "string", "example": "point"},
                    "coords": {"type": "string", "example": "34.866,138.321"}
                }
            }
        ]
        
        return endpoints
    
    def _generate_usage_examples(self, entity_type: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨ä¾‹ã‚’ç”Ÿæˆ"""
        examples = [
            {
                "name": "åŸºæœ¬æ¤œç´¢",
                "description": f"{entity_type}ã®ä¸€è¦§ã‚’å–å¾—",
                "curl_example": f"""curl -X GET "https://api.smartcity-yaizu.jp/v2/entities?type={entity_type}&limit=10" \\
  -H "Fiware-Service: smartcity_yaizu" \\
  -H "Fiware-ServicePath: /{entity_type}" """
            },
            {
                "name": "è¿‘éš£æ¤œç´¢",
                "description": "ç¾åœ¨åœ°ã‹ã‚‰1kmä»¥å†…ã®æ–½è¨­ã‚’æ¤œç´¢",
                "curl_example": f"""curl -X GET "https://api.smartcity-yaizu.jp/v2/entities?type={entity_type}&georel=near;maxDistance:1000&geometry=point&coords=34.866,138.321" \\
  -H "Fiware-Service: smartcity_yaizu" \\
  -H "Fiware-ServicePath: /{entity_type}" """
            }
        ]
        
        return examples
    
    def _find_potential_entity_links(self, entity_type: str) -> List[str]:
        """æ½œåœ¨çš„ãªé–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ç‰¹å®š"""
        relationships = {
            "Aed": ["FirstAidStation", "ReliefHospital", "EvacuationShelter"],
            "EvacuationShelter": ["EvacuationSpace", "Aed", "FirstAidStation"], 
            "WeatherAlert": ["WeatherForecast", "PrecipitationGauge", "StreamGauge"],
            "Event": ["EventDetail", "SightseeingMapStore"],
            "PublicFacility": ["Aed"]
        }
        
        return relationships.get(entity_type, [])
    
    def _assess_data_quality(self, analysis: Dict[str, Any]) -> str:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’è©•ä¾¡"""
        score = 0
        
        if len(analysis.get("extracted_fields", [])) > 0:
            score += 30
        if len(analysis.get("data_types", [])) > 0:
            score += 20
        if len(analysis.get("relationships", [])) > 0:
            score += 20
        if len(analysis.get("examples", [])) > 0:
            score += 15
        if len(analysis.get("constraints", {})) > 0:
            score += 15
            
        if score >= 80:
            return "high"
        elif score >= 50:
            return "medium"
        else:
            return "low"
    
    async def process_all_pdfs(self) -> Dict[str, Any]:
        """å…¨PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦JSONã‚’ç”Ÿæˆ"""
        print("="*60)
        print("PDFâ†’JSON å®Œå…¨å†ç”Ÿæˆãƒ„ãƒ¼ãƒ«")
        print("="*60)
        
        results = {
            "generated_at": datetime.now().isoformat(),
            "total_generated": 0,
            "by_category": {},
            "generated_files": []
        }
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«PDFã‚’å‡¦ç†
        for category_dir in self.documentation_dir.iterdir():
            if category_dir.is_dir() and category_dir.name in self.category_mapping:
                category = category_dir.name
                print(f"\nğŸ“‚ å‡¦ç†ä¸­: {category} ({self.category_mapping[category]})")
                
                pdf_files = list(category_dir.glob("*.pdf"))
                print(f"  ğŸ“„ PDFæ•°: {len(pdf_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
                
                category_results = []
                
                for pdf_file in pdf_files:
                    print(f"  ğŸ”„ ç”Ÿæˆä¸­: {pdf_file.name}")
                    
                    # JSONã‚¹ã‚­ãƒ¼ãƒç”Ÿæˆ
                    json_schema = self.generate_enhanced_json_schema(pdf_file, category)
                    
                    # JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                    json_filename = f"{pdf_file.stem}.json"
                    json_path = self.api_specs_dir / json_filename
                    
                    with open(json_path, 'w', encoding='utf-8') as f:
                        json.dump(json_schema, f, ensure_ascii=False, indent=2)
                    
                    category_results.append({
                        "pdf_file": pdf_file.name,
                        "json_file": json_filename,
                        "entity_type": pdf_file.stem,
                        "file_size": json_path.stat().st_size
                    })
                    
                    print(f"    âœ… ç”Ÿæˆå®Œäº†: {json_filename} ({json_path.stat().st_size:,} bytes)")
                
                results["by_category"][category] = {
                    "category_name": self.category_mapping[category],
                    "pdf_count": len(pdf_files),
                    "generated_count": len(category_results),
                    "files": category_results
                }
                
                results["total_generated"] += len(category_results)
                results["generated_files"].extend(category_results)
        
        # çµ±åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
        index_file = self.api_specs_dir / "index.json" 
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š ç”Ÿæˆå®Œäº†ã‚µãƒãƒªãƒ¼:")
        print(f"  ğŸ“„ ç·ç”Ÿæˆæ•°: {results['total_generated']} ãƒ•ã‚¡ã‚¤ãƒ«")
        for category, info in results["by_category"].items():
            print(f"  ğŸ“‚ {info['category_name']}: {info['generated_count']} ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"  ğŸ“‹ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index_file}")
        print(f"  ğŸ’¾ ä¿å­˜å…ˆ: {self.api_specs_dir}")
        
        return results


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    generator = PDFToJSONGenerator()
    results = await generator.process_all_pdfs()
    
    print(f"\nğŸ‰ PDFâ†’JSONå¤‰æ›ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"ğŸ“Š {results['total_generated']}å€‹ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚")


if __name__ == "__main__":
    asyncio.run(main())